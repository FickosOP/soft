{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "355141bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3f4a9",
   "metadata": {},
   "source": [
    "Method is executed only for first frame, because edges positions are fixed.\n",
    "First we use Canny edge detector to detect edges in a frame.\n",
    "\n",
    "The process of Canny edge detection algorithm can be broken down to five different steps:\n",
    "\n",
    "1. Apply Gaussian filter to smooth the image in order to remove the noise\n",
    "\n",
    "2. Find the intensity gradients of the image\n",
    "\n",
    "3. Apply gradient magnitude thresholding or lower bound cut-off suppression to get rid of spurious response to edge detection\n",
    "\n",
    "4. Apply double threshold to determine potential edges\n",
    "\n",
    "5. Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.[source](https://en.wikipedia.org/wiki/Canny_edge_detector)\n",
    "\n",
    "eg. for video1.mp4 output looks like this:\n",
    "\n",
    "<img src=\"images/canny_edge.jpg\">\n",
    "\n",
    "After this we use HoughLinesP method to get lines from edges. In this case we are looking for lines that are longer than 385.\n",
    "\n",
    "This method finds line segments in a binary image using the probabilistic Hough transform.\n",
    "\n",
    "The basic idea of Hough transform (HT) is to implement a voting procedure for all potential curves in the image, and at the termination of the algorithm, curves that do exist in the image will have relatively high voting scores. Randomized Hough transform (RHT) is different from HT in that it tries to avoid conducting the computationally expensive voting process for every nonzero pixel in the image by taking advantage of the geometric properties of analytical curves, and thus improve the time efficiency and reduce the storage requirement of the original algorithm.[source](https://en.wikipedia.org/wiki/Randomized_Hough_transform)\n",
    "\n",
    "Once we have our lines, we can pinpoint the edges by getting vertical line with lowest X value for left edge, and vertical line with highest X value for right edge.\n",
    "\n",
    "To visualize output we can use cv2.line to draw lines we marked as edges.\n",
    "\n",
    "Output for video1.mp4:\n",
    "<img src=\"images/edges_hough.jpg\">\n",
    "\n",
    "Note that right edge is elevated, because of the rectangle in bottom part of the edge.\n",
    "This will not effect the program since we are comparing center of the ball with X coordinate of the edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca641b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_and_right_edge(frame):\n",
    "    edges = cv2.Canny(frame, 100, 200) #CannyEdge\n",
    "\n",
    "    lines = cv2.HoughLinesP(image=edges, rho=1, theta=np.pi / 180, threshold=10, lines=np.array([]),\n",
    "                            minLineLength=385, maxLineGap=20)  #Extract lines from edges\n",
    "\n",
    "    #Initial value for edges\n",
    "    left_edge = [960, 0, 960, 0]  \n",
    "    right_edge = [0, 0, 0, 0]\n",
    "    if lines is not None:\n",
    "        for i in range(0, len(lines)):\n",
    "            line = lines[i][0]\n",
    "            (x1, y1), (x2, y2) = (line[0], line[1]), (line[2], line[3])\n",
    "    \n",
    "            if x1 == x2:  # condition for vertical line\n",
    "                if x1 < left_edge[0]:\n",
    "                    left_edge = [x1, y1, x2, y2]  # vertical line with minimum X value is left edge\n",
    "                if x1 > right_edge[0]:\n",
    "                    right_edge = [x1, y1, x2, y2] # vertical line with maximum X value is right edge\n",
    "    \n",
    "    #Mark edges on frame\n",
    "    cv2.line(frame, (left_edge[0], left_edge[1]), (left_edge[2], left_edge[3]), (0, 0, 255), 3, cv2.LINE_AA)\n",
    "    cv2.line(frame, (right_edge[0], right_edge[1]), (right_edge[2], right_edge[3]), (0, 0, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "    return left_edge, right_edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58c319",
   "metadata": {},
   "source": [
    "For each frame we get balls positions.\n",
    "\n",
    "First we convert frame to colorless image.\n",
    "Then we apply threshold to gray image to get binary image.\n",
    "After that, we use cv2.findContours to isolate objects from background.\n",
    "Now we need to extract balls from there contours we just obtained.\n",
    "Since balls dimension is fixed, we can filter the list of contours and get only balls, by using method minEnclosingCircle.\n",
    "This method will return center coordinates of the ball and its radius.\n",
    "We save center coordinates to later check whether ball hit the edge or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28bcf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balls(frame):\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    ret, image_bin = cv2.threshold(frame_gray, 150, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(image_bin, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    countours_balls = []\n",
    "    for contour in contours:\n",
    "        (x, y), r = cv2.minEnclosingCircle(contour) # (x, y) are center coordinates r is radius\n",
    "        if 4 > r > 3.5:  # condition to see if contour is actually a ball\n",
    "            countours_balls.append((x, y))\n",
    "    return countours_balls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80882e83",
   "metadata": {},
   "source": [
    "After extraction of balls positions, we need to check if ball hit the edge.\n",
    "We do that by comparing X coordinate of ball center with X coordinate of edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e039bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_contact(left_edge, right_edge, balls):\n",
    "    left_edge_x = left_edge[0]\n",
    "    right_edge_x = right_edge[0]\n",
    "    for ball in balls:  # for each ball in frame we are checking if its X value is close enough to edge to consider it a hit\n",
    "        x = ball[0]\n",
    "        if x - left_edge_x <= 20:\n",
    "            return True\n",
    "        if right_edge_x - x <= 20:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c070d9",
   "metadata": {},
   "source": [
    "Method that iterates through every frame of the video and returns actual count of hits.\n",
    "\n",
    "In order to avoid having one hit recognised as more, we avoid counting hit if it happened within two frames from previous hit.\n",
    "We do this because ball travels different distance with each frame, sometimes it will capture the frame where ball is in direct hit with the edge, and in next frame it will still be close to the edge, so it will count it as two hits. With this we eliminate possibility of that happening.\n",
    "\n",
    "If you want to see the frames where ball hits the edge (and results for each video), please run SOFT3.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4d501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "    frame_num = 0\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(1, frame_num)  # indexing frames\n",
    "\n",
    "    left_edge, right_edge = [], []\n",
    "    hit_counter = 0\n",
    "    # analyze video frame by frame\n",
    "    previous_hit = -1 # variable that is used to avoid counting 1 hit as more(successive frames may be considered hits multiple times)\n",
    "    \n",
    "    while True:\n",
    "        frame_num += 1\n",
    "        ret_val, frame = cap.read()\n",
    "\n",
    "        if not ret_val:\n",
    "            break\n",
    "            \n",
    "        if frame_num == 1:  # for first frame get left and right edge\n",
    "            left_edge, right_edge = get_left_and_right_edge(frame)\n",
    "\n",
    "        balls = get_balls(frame)\n",
    "\n",
    "        hit = check_for_contact(left_edge, right_edge, balls)\n",
    "        \n",
    "        if hit and frame_num > previous_hit + 2:\n",
    "            hit_counter += 1\n",
    "            previous_hit = frame_num\n",
    "\n",
    "    return hit_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bfe05b",
   "metadata": {},
   "source": [
    "For each video we need to check how many hits actually happened. That information is in data/res.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c16039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_count_for_video(file_name, video_name):\n",
    "    file_lbl = open(file_name, \"r\")\n",
    "    for line in file_lbl.readlines():\n",
    "        tokens = line.strip().split(\",\")\n",
    "        if tokens[0] == video_name:\n",
    "            return int(tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c1602",
   "metadata": {},
   "source": [
    "This piece of code shows how we calculate MAE. \n",
    "\n",
    "Mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. MAE is calculated as:\n",
    "\n",
    "MAE = sum(|yi - xi|) / n\n",
    "\n",
    "Where yi is the prediction, xi is true value and n is dataset size. [source](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
    "\n",
    "First we load all videos from data folder.\n",
    "Then we run process_video which returns predicted amount of hits, and we read true amount of hits from res.txt.\n",
    "After that, we use mean_absolute_error method from module sklearn.\n",
    "\n",
    "With this dataset we get MAE = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e81216f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.3\n"
     ]
    }
   ],
   "source": [
    "root_path = \"data\"\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for file in os.listdir(root_path):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".mp4\"):  # get every video from data folder and add it to dataset\n",
    "        dataset.append(file)\n",
    "\n",
    "# lists of real and predicted values to use for calculating mean absolute error\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for video in dataset:\n",
    "    bounces = process_video(root_path + \"/\" + video) # get predicted value\n",
    "    real = get_real_count_for_video(root_path + \"/res.txt\", video) # get real value\n",
    "    y_true.append(real)\n",
    "    y_pred.append(bounces)\n",
    "    \n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1091b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
